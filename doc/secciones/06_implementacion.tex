\chapter{Implementación}
\label{chapter:6}

Este capítulo relata el proceso completo de desarrollo del \textit{scraper} para
TvTropes que resuelve los problemas y cumple los objetivos definidos en el
\autoref{chapter:2}. Se siguen uno a uno los distintos hitos que se han ido
alcanzando, explicando por qué son necesarios y las decisiones que se han tomado
en cada uno de ellos para cumplir las historias de usuario, que guían todo el
desarrollo con agilidad.

En este capítulo se irá describiendo el proceso de desarrollo ágil que se ha
llevado a cabo para poder entender cómo se ha llegado a cada uno de los
sucesivos productos mínimamente viables. A lo largo del proceso se justificarán
una serie de decisiones tomadas, buenas prácticas, recomendaciones y
herramientas que permitirán ir refinando el producto hasta llegar a una solución
final de calidad. Esto implica que todos los hitos se desarrollan con el
principal objetivo de satisfacer al usuario y asegurar la calidad, por lo que
conforme se justifiquen las decisiones tomadas para avanzar en cada uno de
ellos, se relacionarán con las historias de usuario definidas, que son las que
confirman qué se debe implementar para cumplir con las expectativas del usuario.

\section{Comienzo del desarrollo}
Con el primer modelado del problema que se ha realizado en el
\autoref{chapter:5}, tenemos definido todo lo necesario para poder comenzar con
el desarrollo y dar una primera versión de un \textit{scraper} que sea capaz de
extraer la información de \textit{tropos} contenida en páginas individuales de
películas de TvTropes. El primer \textit{milestone} de desarrollo es el
\href{https://github.com/jlgallego99/TropesToGo/milestone/3}{M2}, que resuelve
la parte previa a la extracción. Antes de que el \textit{scraper} extraiga la
información que necesita el usuario, que es el objetivo del siguiente hito,
tiene que determinar si la página sobre la que está trabajando es extraíble, es
decir, pertenece a una página de TvTropes que describe una obra con sus
\textit{tropos} y tiene una estructura HTML conocida. Esto permite determinar,
antes de llevar a cabo el proceso de extracción, que el wiki de TvTropes no ha
cambiado demasiado, lo suficiente para saber si el \textit{scraper} programado
sigue funcionando y así evitar procesamiento innecesario. 

Puesto que se está construyendo un producto mínimo, este hito se centra en que
las páginas de obras que se analicen sean solamente de películas, teniendo un
\textit{milestone} futuro dedicado a ampliarlo a otros tipos de medios
audiovisuales. Solo es necesario comprobar y extraer páginas de un tipo de medio
audiovisual concreto para determinar que el \textit{scraper} completa
correctamente sus tareas, ya que el resto de medios consisten en explorar otras
páginas con una estructura similar y que se obtendrán una vez se tenga una araña
que encuentre ese tipo de páginas. Se han elegido páginas de películas por ser
el medio en el que se centran la mayoría de trabajos de investigación estudiados
en la introducción y el estado del arte, y son las que más interés tienen para
una primera versión funcional.

\subsection{Flujo de integración continua para pruebas y compilación}
En este hito se empieza a programar código funcional con lógica de negocio, así
que se comienzan también a desarrollar conjuntamente los tests, concretamente
antes de la propia funcionalidad, tal y como se especifica en el desarrollo
dirigido por pruebas \cite{beck2002driven} y se definió en el
\autoref{chapter:4}. Por tanto, uno de los objetivos de este hito es tener
integrado en el código del proyecto el \textit{framework} de pruebas Ginkgo, que
es el que permitirá desarrollar los tests en Go, para que en los próximos hitos
se puedan desarrollar todas las pruebas para testear las nuevas funcionalidades.
Además, al estar en un entorno de desarrollo ágil la ejecución de las pruebas
estará automatizada, de forma que se tenga un flujo de integración continua que
ejecute automáticamente todos los tests definidos y se pueda comprobar desde el
repositorio de GitHub. 

En cada uno de los hitos, cuando se desarrolla nueva funcionalidad, se abre un
\textit{pull request} en GitHub con los cambios realizados en el proyecto para
saber qué incremento en el producto se está haciendo y qué \textit{issues}
pretende resolver para avanzar en completar el hito. Cada \textit{issue}
representa cada uno de los problemas que surgen durante el desarrollo y que se
intentan resolver para avanzar en completar una historia de usuario. Estos son
los problemas que se justifican y resuelven a lo largo de este capítulo. Los
nuevos flujos de CI que se añaden en este hito permiten automatizar la ejecución
de las pruebas y compilar todo el proyecto, para cumplir que todo el código sea
funcional y asegurar su calidad. Si cualquiera de estos flujos de CI fallan, no
se podrá aceptar el \textit{pull request} y añadir los cambios a la rama
principal. La integración continua en este proyecto se configura mediante
\textit{GitHub Actions}, que automatizan los procesos apoyándose en nuevas
tareas definidas mediante el gestor de tareas \textit{Mask}. Estas tareas
ejecutan los tests para todo el proyecto, ambas usando por debajo la propia
utilidad del lenguaje Go: \texttt{go test} para las pruebas y \texttt{go build}
para comprobar que todos los paquetes del código compilan correctamente. Esto
permite que no haga falta modificarlos en el futuro; servirán para todo el
desarrollo de ahora en adelante y ejecutarán automáticamente todas las nuevas
pruebas que se programen. En el caso de necesitar algún cambio en la ejecución
de los tests o la compilación, bastará con cambiar las tareas del gestor de
tareas, sin necesidad de tocar la configuración de la integración continua.

Por tanto, para poder cumplir con la automatización de las pruebas y la
compilación se configura un nuevo \textit{GitHub Action} en el que se prepara
Go, se instala el gestor de tareas \textit{Mask} y se ejecutan las tareas
definidas para la compilación y el testeo. En un principio se probó a instalar
\textit{Mask} mediante \textit{cargo}, el gestor de paquetes del lenguaje Rust,
ya que la máquina virtual que utiliza el \textit{Action} es de Ubuntu y su
gestor de paquetes no tiene disponible este gestor de tareas en su repositorio
para instalarlo. Sin embargo, esta manera de instalar el gestor de tareas
suponía una gran sobrecarga en el flujo de trabajo al tener que instalar todo el
lenguaje Rust únicamente para instalar un paquete, tardando casi 2 minutos en
ejecutarlo por completo, gastando la mayoría de recursos en instalar el gestor
de tareas. Para solucionar esto se acabó optando por introducir los comandos en
el flujo de CI necesarios para descargar directamente el binario de
\textit{Mask}; esto hizo que se mejorase el tiempo en más de la mitad, ahorrando
los recursos de GitHub y teniendo el resultado del flujo de CI mucho antes.
Adicionalmente, una de las ventajas que tiene el usar Go es que, al ejecutar
cualquier comando de compilación, testeo o ejecución del código, comprueba
automáticamente si las dependencias definidas en el fichero \texttt{go.mod}
están instaladas, y si no lo están las descarga automáticamente, por lo que no
hace falta especificar explícitamente qué dependencias hay que instalar para que
el código funcione.

\subsection{Comprobación de la estructura de una página de TvTropes}
En este hito se modifica el servicio \textit{scraper} añadiendo la funcionalidad
necesaria para cumplir su objetivo: verificar que una página de una obra de
TvTropes no ha cambiado y se puede extraer. Un servicio en DDD hace referencia a
un paquete de código que ejecuta cierta lógica de negocio para un cliente y en
este caso el cliente sería el usuario que necesita los datos. 

El objetivo de este hito se alcanza implementando la función
\texttt{CheckValidWorkPage} que encapsula la funcionalidad de revisar toda la
estructura de una página de película de TvTropes y validar si el
\textit{scraper} podrá extraerla. Recibe una entidad \textit{Page}, de la cual
el \textit{scraper} extrae lo principal que necesita, que es el URL de la página
para poder hacerle una petición y obtener su código HTML. La razón de recibir la
página por referencia se debe a que la página es mutable, es posible que se haya
actualizado y el \textit{scraper} necesite en el futuro cambiar su atributo de
última modificación, además de que será más eficiente para el \textit{scraper}
el obtener las páginas por referencia que previamente habrá creado el
\textit{crawler} en hitos posteriores. En general, en DDD se tratan las
entidades siempre como referencias, puesto que son mutables, mientras que los
objetos valor se tratarán como variables instanciadas. 

Esta función se ha modularizado en distintas subfunciones que realizan
revisiones de distintos tipos, cumpliendo la propiedad de única responsabilidad
de cada una de las funciones y haciéndolas más legibles. Esto facilita que, en
caso de que se necesitasen analizar nuevas partes, se modifiquen las sub
funciones ya existentes o se añada una nueva, que simplemente se llamaría desde
el método principal. Según \textit{clean code} no se deben tener funciones que
mezclen llamadas a otras funciones de alto nivel con lógica más compleja, por lo
que el método principal \texttt{CheckValidWorkPage} se encarga de llamar a las
distintas sub funciones que llevan a cabo verificaciones de distintos tipos y
trata sus resultados para determinar finalmente si la información de la página
es extraíble o no.

Estas comprobaciones se efectúan en orden de menor a mayor complejidad y de
mayor a menor nivel, intentando que el filtrado se lleve a cabo cuanto antes
para evitar entrar en análisis complejos de páginas innecesarias. Se busca poder
validar la página lo antes posible, sin perder mucho tiempo en el proceso para
cumplir la \href{https://github.com/jlgallego99/TropesToGo/issues/45}{[HU06]} lo
mejor posible y que el usuario obtenga cuanto antes el resultado. Primero se
determina si la página pertenece a TvTropes, luego se analiza la estructura
general del artículo principal de la página y finalmente se comprueba la sección
donde deberían estar contenidos los \textit{tropos}.
\begin{itemize}
    \item Para verificar que la página es de TvTropes se extrae el URL de la
    página y se compara si el \textit{host} es \url{tvtropes.org} y si el URI
    corresponde a lo que entendemos como una página de película, es decir,
    pertenece a \textit{Main} y está bajo el índice \textit{Film}. La penúltima
    parte de la ruta tiene que ser el índice al que pertenece la obra, en este
    caso \textit{Film}, y la última parte será el nombre de la película. Esto es
    lo primero que se debe hacer; si la página que se está analizando no es de
    TvTropes y/o no pertenece a una obra, no tiene sentido seguir adelante con
    ella. Sin embargo, puede ser que su estructura interna haya cambiado, por lo
    que es necesario estudiarla más a fondo.
    \item Una vez se sabe que la página es de TvTropes, es necesario ver si
    pertenece a una página de obra y su HTML interno no ha cambiado, ya que es
    de donde luego el \textit{scraper} extraerá la información. Principalmente,
    se revisa en el árbol DOM si la página tiene las partes vistas en el
    análisis de la \autoref{fig:tvtropes-work}: un artículo principal con sus
    identificadores y clases conocidas y si el índice, dentro del título del
    artículo, pertenece al de películas. Todas las páginas de obra,
    independientemente de si son películas o no, siguen esta misma estructura,
    por lo que esto basta para confirmar si este tipo de páginas siguen igual.
    \item Por último, se realizan las verificaciones más complejas relacionadas
    con la principal sección que se querrá analizar: la sección de
    \textit{tropos}. Debido al estudio de la \autoref{fig:tropelist} del
    \autoref{chapter:5} sabemos que existen tres formas comunes que toma la
    sección de \textit{tropos}, y esto es lo que el \textit{scraper} verifica,
    para asegurarnos de que podemos entender la sección. En este primer hito
    solo se tienen en cuenta los tres tipos, ya que bastan para conformar un
    producto mínimo que sea capaz de analizar la mayoría de páginas de TvTropes,
    sin embargo, en futuros hitos se tendrán en cuenta casos más extremos.\\
    Se sigue la misma estrategia de buscar primero lo más relevante, que en este
    caso es encontrar la sección donde se encuentran los \textit{tropos}, la
    cual está conformada por una lista sin ordenar dentro del artículo principal
    y que está siempre después del resumen y precedida de una etiqueta de
    \textit{header}. Se ignora el comprobar el texto de la cabecera de la
    sección de \textit{tropos}, ya que como se vio en \cite{nishalscraping} el
    contenido cambia mucho y es poco fiable, siendo lo único fiable el hecho de
    que la sección está separada por una cabecera cualquiera. Una vez confirmado
    que la página tiene esa sección, toca ver si se adapta a uno de los tres
    tipos que se conocen. Para probar si los \textit{tropos} están contenidos en
    carpetas basta con buscar el botón de abrir carpetas, que sabemos su
    etiqueta, clase y función JavaScript que utiliza para abrirlas, al estar
    generado siempre igual por el motor del wiki. En todos los casos se valida
    si el primer elemento de la lista es un enlace a una página de
    \textit{tropos} o a una sub página de ellos. Para este último caso, que
    pertenecería al tercer tipo, se ha hecho uso de una expresión regular que
    confirma si el URI al que redirigen sigue un patrón concreto. Si la última
    parte del URI está codificada como la cadena
    \texttt{/<NombreDeLaObra>/Tropes<XtoY>} entonces se acepta como algo
    conocido. Si la lista contiene cualquier otro elemento inesperado, se
    considera que la página tiene una estructura desconocida y no se puede
    extraer su información.
\end{itemize}

La función de comprobación devuelve un valor booleano, que indica si la página
es extraíble o no, y un error que indica el por qué no es extraíble en caso de
no serlo. En general, es idiomático en Go que todas las funciones devuelvan
siempre un error como último valor para que el código sea autoexplicativo y se
tenga un completo y correcto control de los errores tanto para que las funciones
de testing puedan comprobar todos los casos como para que cualquier cliente que
llame a la función del servicio entienda exactamente qué ha pasado en caso de
error \cite{effective_go}.

Finalmente, en este hito se toma la decisión de que los selectores CSS, los
cuales son una cadena de \textit{string} que hacen referencia a varias etiquetas
HTML y clases, se encapsularán en valores constantes. Esto sigue las guías de
\textit{clean code} sobre evitar los literales, facilitando la legibilidad del
código y evitando la repetición. De este modo, el \textit{scraper} pasará como
parámetro estas constantes a las funciones de Goquery que encuentran los nodos
del árbol DOM de la página que se está explorando. A estas constantes se les da
un nombre lo más semántico posible de forma que el código sea legible y limpio.
Por ejemplo, la constante \texttt{TropeTag} contiene el selector
\texttt{a.twikilink} que hace referencia a todos los \textit{anchor} de HTML
cuya clase es \textit{twikilink}, que sabemos debido al análisis del capítulo
anterior que hacen referencia a enlaces hacia páginas de \textit{tropos}. El
tener constantes con selectores importantes para lo que quiere buscar el
\textit{scraper} también implica otras ventajas como el poder combinar distintos
selectores de modo que se entienda su significado completo con solo leerlo o el
poder adaptarse a los futuros y posibles cambios que pueda sufrir TvTropes. En
el caso de que cambie alguna clase o etiqueta con la que encontrar alguna
sección valiosa de una página, baste con cambiar la constante y no cambiarlo en
cada parte del código en la que aparezca. En general, al usar constantes
semánticas para los selectores se facilita mucho la legibilidad de algo que de
otro modo requeriría de un análisis más exhaustivo para entender qué significa
el selector utilizado, ya que pueden volverse muy complejos al unir varias
reglas.

\section{Extracción de información en la página principal de una película}
Una vez desarrollada toda la funcionalidad necesaria para comprobar si una
página es de TvTropes y es entendible por el \textit{scraper}, el siguiente paso
lógico es comenzar con el propio proceso de \textit{scraping}, al ser la parte
principal que necesita resolver TropesToGo. La estrategia a seguir en el proceso
de desarrollo es la de centrarse primero en la construcción de uno o más
productos mínimamente viables que culminen en un \textit{scraper} que pueda
sacar toda la información que piden los usuarios según la
\href{https://github.com/jlgallego99/TropesToGo/issues/6}{[HU01]} y
representarlos en un conjunto de datos apto para su uso en procesos de ciencia
de datos, según las historias de usuario
\href{https://github.com/jlgallego99/TropesToGo/issues/30}{[HU05]} y
\href{https://github.com/jlgallego99/TropesToGo/issues/46}{[HU07]}, que
especifican que el usuario necesita acceder al resultado de la extracción en
cualquier momento, de forma offline y en un formato estandarizado. Lo que
produce el \textit{scraper} es el \textit{dataset}; el resultado final que
ofrece TropesToGo al usuario para cumplir sus necesidades, el cual podrá estar
representado en más de un formato ampliamente utilizado por investigadores. Una
vez se tenga un \textit{scraper} totalmente funcional, se podrá avanzar en
futuros \textit{milestones} en el desarrollo de la araña que generalice el
proceso de \textit{scraping} a toda la web de TvTropes completa. 

Por tanto, en este \textit{milestone}
\href{https://github.com/jlgallego99/TropesToGo/milestone/3}{M2} se pretende
ampliar el servicio \textit{scraper} para que pueda extraer los datos presentes
en el artículo principal de varias páginas de prueba. En un principio se tenía
pensado un único hito donde desarrollar el \textit{scraper} completo, capaz de
extraer toda la información necesaria de cualquier película. Sin embargo, al
observar la complejidad que tienen algunas páginas y a lo dispersos que están
los \textit{tropos} de una obra, esa primera idea se dividió en dos productos
mínimos, que son el desarrollado en este hito y en el siguiente. Por tanto, en
este \textit{milestone} el principal objetivo es extraer la información
contenida en la página principal de una película, es decir: su título, año de
salida, índice al que pertenece (el tipo de \textit{media}, o medio audiovisual,
que en este caso será \textit{Film} (película)) y su lista principal de
\textit{tropos}. Existe más información que no se encuentra en el artículo
principal y que se analizará en el siguiente hito, principalmente de
\textit{tropos} adicionales, que está en otras sub páginas de la obra o en
páginas de otros tipos.

Como lista principal entendemos los \textit{tropos} que aparecen directamente
listados al final del artículo, como se podía ver en la
\autoref{fig:tvtropes-work}, que se consideran los más importantes de la obra al
estar en primer plano. Puesto que este \textit{milestone} está centrado en
extraer información del artículo principal únicamente, se extraerán solo los
\textit{tropos} que están contenidos en listas o carpetas, tal y como se ve en
los dos primeros tipos de la \autoref{fig:tropelist}, ignorando por lo pronto el
tercer tipo que, aunque también tiene los \textit{tropos} principales de la
obra, implicaría ahondar en sub páginas. En el próximo \textit{milestone} se
navegará a estas sub páginas y se discutirán las dis´tintas fuentes donde se
pueden encontrar otros \textit{tropos} que no sean los principales.

\subsection{Ampliación del \textit{scraper}}


\subsection{Implementación de repositorios para manejar los \textit{datasets}}


\section{Desarrollo de la araña}

\subsection{Estrategia general para la extracción de información de TvTropes}
El \textit{crawler} primero indexa todas las páginas relevantes, el
\textit{scraper} luego extrae la información y almacena en un fichero, etc.

\subsection{Arquitectura del \textit{crawler}}

\section{Aplicación de línea de comandos}